{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The text.latex.unicode rcparam was deprecated in Matplotlib 3.0 and will be removed in 3.2.\n",
      "In C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The savefig.frameon rcparam was deprecated in Matplotlib 3.1 and will be removed in 3.3.\n",
      "In C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The pgf.debug rcparam was deprecated in Matplotlib 3.0 and will be removed in 3.2.\n",
      "In C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The verbose.level rcparam was deprecated in Matplotlib 3.1 and will be removed in 3.3.\n",
      "In C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The verbose.fileo rcparam was deprecated in Matplotlib 3.1 and will be removed in 3.3.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import random\n",
    "import glob\n",
    "import os\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "import time\n",
    "keras = tf.keras\n",
    "layers = tf.keras.layers\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(path):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [224, 224])\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = image/255.0  # normalize to [0,1] range\n",
    "    image = 2*image-1\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r'F:\\weightbodysize\\data\\train'\n",
    "checkpoint_path = 'F:/weightbodysize/addblDenseNet201.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = pathlib.Path(data_dir)\n",
    "all_image_paths = list(data_root.glob('*/*'))\n",
    "all_image_paths = [str(path) for path in all_image_paths]\n",
    "random.shuffle(all_image_paths)\n",
    "all_image_labels = [pathlib.Path(path).parent.name for path in all_image_paths]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_labels = [[label.split('_')[1]] for label in all_image_labels]\n",
    "new_weight_labels=[]\n",
    "for n in weight_labels:\n",
    "    new_weight_labels.append(float(n[0]));  \n",
    "weight_labels=new_weight_labels\n",
    "\n",
    "bw_labels = [[label.split('_')[2]] for label in all_image_labels]\n",
    "new_bw_labels=[]\n",
    "for n in bw_labels:\n",
    "    new_bw_labels.append(float(n[0]));    \n",
    "bw_labels=new_bw_labels\n",
    "\n",
    "bh_labels = [[label.split('_')[3]] for label in all_image_labels]\n",
    "new_bh_labels=[]\n",
    "for n in bh_labels:\n",
    "    new_bh_labels.append(float(n[0]));    \n",
    "bh_labels=new_bh_labels\n",
    "\n",
    "hw_labels = [[label.split('_')[4]] for label in all_image_labels]\n",
    "new_hw_labels=[]\n",
    "for n in hw_labels:\n",
    "    new_hw_labels.append(float(n[0]));    \n",
    "hw_labels=new_hw_labels\n",
    "\n",
    "hh_labels = [[label.split('_')[5]] for label in all_image_labels]\n",
    "new_hh_labels=[]\n",
    "for n in hh_labels:\n",
    "    new_hh_labels.append(float(n[0]));    \n",
    "hh_labels=new_hh_labels\n",
    "\n",
    "bl_labels = [[label.split('_')[6]] for label in all_image_labels]\n",
    "new_bl_labels=[]\n",
    "for n in bl_labels:\n",
    "    new_bl_labels.append(float(n[0]));    \n",
    "bl_labels=new_bl_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ds = tf.data.Dataset.from_tensor_slices(all_image_paths)\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "image_ds = path_ds.map(load_and_preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "label_ds = tf.data.Dataset.from_tensor_slices((weight_labels, bw_labels, bh_labels, hw_labels, hh_labels, bl_labels))\n",
    "image_label_ds = tf.data.Dataset.zip((image_ds, label_ds))\n",
    "image_count = len(all_image_paths)\n",
    "\n",
    "test_count = int(image_count*0.3)\n",
    "train_count = image_count - test_count\n",
    "train_data = image_label_ds.skip(test_count)\n",
    "test_data = image_label_ds.take(test_count)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "train_data = train_data.shuffle(buffer_size=train_count).repeat(-1)\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "train_data = train_data.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "test_data = test_data.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DenseNet201 = keras.applications.densenet.DenseNet201(weights='imagenet',include_top=False,input_shape=(224, 224, 3))\n",
    "DenseNet201.trainable = True\n",
    "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "x = DenseNet201(inputs)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "out_weight = tf.keras.layers.Dense(1,name='out_weight')(x)\n",
    "out_bw = tf.keras.layers.Dense(1,name='out_bw')(x)\n",
    "out_bh = tf.keras.layers.Dense(1,name='out_bh')(x)\n",
    "out_hw = tf.keras.layers.Dense(1,name='out_hw')(x)\n",
    "out_hh = tf.keras.layers.Dense(1,name='out_hh')(x)\n",
    "out_bl = tf.keras.layers.Dense(1,name='out_bl')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs,\n",
    "                       outputs=[out_weight, out_bw, out_bh, out_hw, out_hh, out_bl])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('start time:',time.asctime( time.localtime(time.time()) ))\n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss='mse')\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,save_best_only=True,monitor='val_loss')\n",
    "history = model.fit(train_data,\n",
    "                    steps_per_epoch=train_count//BATCH_SIZE,\n",
    "                    epochs=150,\n",
    "                    validation_data=test_data,\n",
    "                    validation_steps=test_count//BATCH_SIZE,\n",
    "                   callbacks=[cp_callback])\n",
    "np.savetxt('DenseNet201_loss.txt',history.history['loss'])\n",
    "#np.savetxt('DenseNet201_acc.txt',history.history['acc'])\n",
    "np.savetxt('DenseNet201_val_loss.txt',history.history['val_loss'])\n",
    "#np.savetxt('DenseNet201_val_acc.txt',history.history['val_acc'])\n",
    "print ('end time:',time.asctime( time.localtime(time.time()) ))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
